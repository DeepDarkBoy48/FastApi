from google import genai
from google.genai import types
from pydantic import BaseModel, Field
from typing import List, Optional
import os
import json
from dotenv import load_dotenv
from schemas import (
    AnalysisResult, AnalysisRequest,
    DictionaryResult, LookupRequest,
    WritingResult, WritingRequest, WritingMode,
    ChatRequest
)

try:
    load_dotenv()
except Exception as e:
    print(f"Warning: Could not load .env file: {e}")

api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    print("Warning: GEMINI_API_KEY not found in environment. AI features will fail.")
    api_key = "MISSING_API_KEY"

client = genai.Client(api_key=api_key)

# --- Existing Subtitle Logic ---
class SubtitleItem(BaseModel):
    start: str = Field(description="Original start timestamp string (e.g., '00:01:23.456'). Do NOT convert to seconds.")
    end: str = Field(description="Original end timestamp string (e.g., '00:01:25.789'). Do NOT convert to seconds.")
    text: str = Field(description="Complete, merged sentence text.")

class SubtitlesResponse(BaseModel):
    subtitles: List[SubtitleItem]

async def get_response(prompt):
    response = await client.aio.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt,
        config=types.GenerateContentConfig(
            response_mime_type="application/json",
            response_schema=SubtitlesResponse,
            thinking_config=types.ThinkingConfig(
                thinking_budget=0
            ),
            system_instruction="""
                    # Role
                    You are an expert subtitle editor. 
                    
                    # Task
                    1. Merge fragmented sentences into complete sentences based on context.
                    2. Deduplicate repeating lines.
                    3. Keep timestamps in their **ORIGINAL format** (HH:MM:SS.mmm).
                    
                    # Rules
                    - strictly maintain the timeline sequence.
                    - Start time = start timestamp of the first fragment.
                    - End time = end timestamp of the last fragment.
                    - Do NOT convert times to math/floats. Just copy the string.
            """),
    )
    return response.parsed.subtitles

# --- SmashEnglish Logic ---

def get_model_config():
    # Default to a balanced configuration
    return 'gemini-2.5-flash', 200

async def analyze_sentence_service(sentence: str) -> AnalysisResult:
    model, thinking_budget = get_model_config()
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä½ç²¾é€šè¯­è¨€å­¦å’Œè‹±è¯­æ•™å­¦çš„ä¸“å®¶ AIã€‚è¯·åˆ†æä»¥ä¸‹è‹±è¯­å¥å­ï¼š "{sentence}"ã€‚
    ç›®æ ‡å—ä¼—æ˜¯æ­£åœ¨å­¦ä¹ è‹±è¯­çš„å­¦ç”Ÿï¼Œå› æ­¤åˆ†æéœ€è¦**æ¸…æ™°ã€å‡†ç¡®ä¸”å…·æœ‰æ•™è‚²æ„ä¹‰**ã€‚

    **Processing Steps (Thinking Process):**
    1.  **Grammar Check (çº é”™)**: 
        - ä»”ç»†æ£€æŸ¥å¥å­æ˜¯å¦æœ‰è¯­æ³•é”™è¯¯ã€‚
        - å¦‚æœæœ‰é”™ï¼Œåˆ›å»ºä¸€ä¸ªä¿®æ­£åçš„ç‰ˆæœ¬ã€‚
        - **æ³¨æ„**ï¼šåç»­çš„æ‰€æœ‰åˆ†æï¼ˆchunks, detailedTokens, structureï¼‰å¿…é¡»åŸºäº**ä¿®æ­£å(Corrected)** çš„å¥å­è¿›è¡Œã€‚
        - **Diff Generation**: ç”Ÿæˆ 'changes' æ•°ç»„æ—¶ï¼Œå¿…é¡»æ˜¯ä¸¥æ ¼çš„æ–‡æœ¬å·®å¼‚å¯¹æ¯” (diff)ã€‚
          - 'remove': ä»…åŒ…å«è¢«åˆ é™¤çš„åŸæ–‡ç‰‡æ®µï¼Œ**ç»å¯¹ä¸è¦**åŒ…å« "->" ç¬¦å·æˆ– "change x to y" è¿™æ ·çš„æè¿°ã€‚ä¾‹å¦‚åŸå¥æ˜¯ "i go"ï¼Œä¿®æ­£ä¸º "I go"ï¼Œåˆ™ 'remove' text ä¸º "i"ï¼Œ'add' text ä¸º "I"ã€‚
          - 'add': ä»…åŒ…å«æ–°åŠ å…¥çš„ç‰‡æ®µã€‚
          - 'keep': ä¿æŒä¸å˜çš„éƒ¨åˆ†ã€‚

    2.  **Macro Analysis (å®è§‚ç»“æ„)**:
        - è¯†åˆ«æ ¸å¿ƒå¥å‹ç»“æ„ (Pattern)ï¼Œ**å¿…é¡»åŒ…å«ä¸­æ–‡ç¿»è¯‘**ã€‚æ ¼å¼è¦æ±‚ï¼š"English Pattern (ä¸­æ–‡åç§°)"ã€‚ä¾‹å¦‚ï¼š"S + V + O (ä¸»è°“å®¾)"ã€‚
        - è¯†åˆ«æ ¸å¿ƒæ—¶æ€ (Tense)ï¼Œ**å¿…é¡»åŒ…å«ä¸­æ–‡ç¿»è¯‘**ã€‚æ ¼å¼è¦æ±‚ï¼š"English Tense (ä¸­æ–‡åç§°)"ã€‚ä¾‹å¦‚ï¼š"Present Simple (ä¸€èˆ¬ç°åœ¨æ—¶)"ã€‚

    3.  **Chunking (å¯è§†åŒ–æ„ç¾¤åˆ†å—)**:
        - ç›®æ ‡æ˜¯å±•ç¤ºå¥å­çš„â€œèŠ‚å¥â€å’Œâ€œæ„ç¾¤â€(Sense Groups)ã€‚
        - **åŸåˆ™**ï¼š
          - æ‰€æœ‰çš„ä¿®é¥°è¯­åº”ä¸å…¶ä¸­å¿ƒè¯åœ¨ä¸€èµ·ï¼ˆä¾‹å¦‚ "The very tall man" æ˜¯ä¸€ä¸ªå—ï¼‰ã€‚
          - ä»‹è¯çŸ­è¯­é€šå¸¸ä½œä¸ºä¸€ä¸ªæ•´ä½“ï¼ˆä¾‹å¦‚ "in the morning" æ˜¯ä¸€ä¸ªå—ï¼‰ã€‚
          - è°“è¯­åŠ¨è¯éƒ¨åˆ†åˆå¹¶ï¼ˆä¾‹å¦‚ "have been waiting" æ˜¯ä¸€ä¸ªå—ï¼‰ã€‚
          - ä¸å®šå¼çŸ­è¯­åˆå¹¶ï¼ˆä¾‹å¦‚ "to go home" æ˜¯ä¸€ä¸ªå—ï¼‰ã€‚

    4.  **Detailed Analysis (é€è¯/çŸ­è¯­è¯¦è§£)**:
        - **æ ¸å¿ƒåŸåˆ™ - å›ºå®šæ­é…ä¼˜å…ˆ**ï¼š
          - é‡åˆ°çŸ­è¯­åŠ¨è¯ (phrasal verbs)ã€å›ºå®šä¹ è¯­ (idioms)ã€ä»‹è¯æ­é… (collocations) æ—¶ï¼Œ**å¿…é¡»**å°†å®ƒä»¬ä½œä¸ºä¸€ä¸ªæ•´ä½“ Tokenï¼Œ**ç»å¯¹ä¸è¦æ‹†åˆ†**ã€‚
          - ä¾‹å¦‚ï¼š"look forward to", "take care of", "a cup of", "depend on"ã€‚
          - **ç‰¹åˆ«å¤„ç†å¯åˆ†ç¦»çŸ­è¯­åŠ¨è¯ (Separable Phrasal Verbs)**ï¼š
            - å¦‚æœé‡åˆ°åƒ "pop us back", "turn it on" è¿™æ ·åŠ¨è¯ä¸å°å“è¯è¢«ä»£è¯éš”å¼€çš„æƒ…å†µï¼Œè¯·åŠ¡å¿…**è¯†åˆ«å‡ºå…¶æ ¸å¿ƒçŸ­è¯­åŠ¨è¯**ï¼ˆå¦‚ "pop back"ï¼‰ã€‚
            - åœ¨è¯¦ç»†è§£é‡Š (explanation) ä¸­ï¼Œ**å¿…é¡»**æ˜ç¡®æŒ‡å‡ºè¯¥è¯å±äºçŸ­è¯­åŠ¨è¯ "pop back" (æˆ–ç›¸åº”çŸ­è¯­)ï¼Œå¹¶è§£é‡Šè¯¥çŸ­è¯­åŠ¨è¯çš„å«ä¹‰ï¼Œè€Œä¸ä»…ä»…æ˜¯å•ä¸ªå•è¯çš„æ„æ€ã€‚
            - ç¤ºä¾‹ï¼šé’ˆå¯¹ "pop us back"ï¼Œåœ¨è§£é‡Š "pop" æ—¶ï¼Œåº”è¯´æ˜ "pop ... back æ˜¯çŸ­è¯­åŠ¨è¯ï¼Œæ„ä¸ºè¿…é€Ÿå›å»/æ”¾å›"ã€‚
        - **è§£é‡Š (Explanation)**ï¼š
          - ä¸è¦åªç»™ä¸€ä¸ªè¯æ€§æ ‡ç­¾ã€‚è¦è§£é‡Šå®ƒåœ¨å¥å­ä¸­çš„**åŠŸèƒ½**å’Œ**ä¸ºä»€ä¹ˆç”¨è¿™ç§å½¢å¼**ã€‚
          - ä¾‹å¦‚ï¼šä¸è¦åªå†™"è¿‡å»åˆ†è¯"ï¼Œè¦å†™"è¿‡å»åˆ†è¯ï¼Œä¸ has æ„æˆç°åœ¨å®Œæˆæ—¶ï¼Œè¡¨ç¤ºåŠ¨ä½œå·²å®Œæˆ"ã€‚
        - **å«ä¹‰ (Meaning)**ï¼šæä¾›åœ¨å½“å‰è¯­å¢ƒä¸‹çš„ä¸­æ–‡å«ä¹‰ã€‚

    è¯·è¿”å› JSON æ ¼å¼æ•°æ®ã€‚
    """

    try:
        response = await client.aio.models.generate_content(
            model=model,
            contents=prompt,
            config=types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=AnalysisResult,
                thinking_config=types.ThinkingConfig(thinking_budget=thinking_budget) if thinking_budget > 0 else None,
            )
        )
        
        if not response.parsed:
             raise ValueError("Empty response from Gemini")
             
        result = response.parsed
        # Match frontend logic: use corrected sentence if available
        if result.correction:
            result.englishSentence = result.correction.corrected
        else:
            result.englishSentence = sentence
            
        return result
    except Exception as e:
        print(f"Gemini API Error: {e}")
        raise Exception("æ— æ³•åˆ†æè¯¥å¥å­ã€‚è¯·æ£€æŸ¥ç½‘ç»œæˆ– API Key è®¾ç½®ã€‚")


async def lookup_word_service(word: str) -> DictionaryResult:
    model, thinking_budget = get_model_config()

    prompt = f"""
    Act as a professional learner's dictionary specifically tailored for students preparing for **IELTS, TOEFL, and CET-6**.
    User Look-up Query: "{word}".
    
    **STEP 1: Normalization & Generalization (CRITICAL)**
    1. Analyze the user's input. Is it a specific instance of a phrasal verb or collocation with specific pronouns?
    2. If yes, convert it to the **Canonical Form** (Headword).
       - Input: "pop us back" -> Output: "pop sth back"
       - Input: "made up my mind" -> Output: "make up one's mind"
    
    **STEP 2: Filtering & Content Generation**
    1. **Target Audience**: Students preparing for exams (IELTS, TOEFL, CET-6) and daily communication.
    2. **Filtering Rule**: 
       - OMIT rare, archaic, obsolete, or highly technical scientific definitions unless the word itself is technical.
       - Focus ONLY on the most common 3-4 meanings used in modern English and exams.
    3. **COCA Frequency per Part of Speech**:
       - For each part of speech (e.g. Noun vs Verb), estimate its specific COCA frequency rank.
       - Example: "address" might be "Rank 1029" as a Noun, but "Rank 1816" as a Verb.
       - Provide a concise string like "Rank 1029" or "Top 2000".

    **STEP 3: Structure**
    - Definitions: Clear, simple English explanation + Concise Chinese meaning.
    - Examples: Must be natural, modern, and relevant to exam contexts or daily life.
    
    **STEP 4: Collocations & Fixed Phrases**
    - Identify 3-5 high-frequency collocations, idioms, or fixed phrases containing this word.
    - Prioritize phrases useful for IELTS/TOEFL writing or speaking.
    - Provide meaning and a sentence example for each.

    Structure the response by Part of Speech (POS).
    Return strictly JSON.
    """

    try:
        response = await client.aio.models.generate_content(
            model=model,
            contents=prompt,
            config=types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=DictionaryResult,
                thinking_config=types.ThinkingConfig(thinking_budget=thinking_budget) if thinking_budget > 0 else None,
            )
        )
        
        if not response.parsed:
             raise ValueError("Empty response from Gemini")
        
        return response.parsed
    except Exception as e:
        print(f"Dictionary API Error: {e}")
        raise Exception("æ— æ³•æŸ¥è¯¢è¯¥å•è¯ï¼Œè¯·é‡è¯•ã€‚")


async def evaluate_writing_service(text: str, mode: WritingMode) -> WritingResult:
    model, thinking_budget = get_model_config()

    mode_instructions = """
    **MODE: BASIC CORRECTION (åŸºç¡€çº é”™)**
    - Target: General accuracy.
    - Task: Focus STRICTLY on correcting grammar, spelling, punctuation, and serious awkwardness.
    - Do NOT change style, tone, or vocabulary unless it is incorrect.
    - Keep the output very close to the original, only fixing errors.
    """

    prompt = f"""
    Act as a professional English Editor and IELTS Examiner.
    
    {mode_instructions}

    **Task**:
    Analyze the user's text and reconstruct it into the *Improved Version* according to the selected mode.
    You must return the result as a sequence of SEGMENTS that allow us to reconstruct the full text while highlighting exactly what changed.

    **Input Text**: "{text}"

    **Output Logic**:
    - Iterate through the improved text.
    - If a part of the text is the same as original, mark it as 'unchanged'.
    - If you changed, added, or removed something, create a segment of type 'change'.
      - 'text': The NEW/IMPROVED text.
      - 'original': The ORIGINAL text that was replaced (or empty string if added).
      - 'reason': A brief explanation in Chinese.
      - 'category': One of 'grammar', 'vocabulary', 'style', 'punctuation', 'collocation' | 'punctuation'.
    - **CRITICAL - PARAGRAPH PRESERVATION**: 
      - You MUST preserve all paragraph breaks and newlines (\\n) from the original text exactly as they are.
      - When you encounter a newline in the original text, return it as a separate segment: {{ "text": "\\n", "type": "unchanged" }}.
      - Do NOT merge paragraphs.
    
    **Example**:
    Original: "I go store.\\n\\nIt was fun."
    Improved: "I went to the store.\\n\\nIt was fun."
    Segments:
    [
      {{ "text": "I ", "type": "unchanged" }},
      {{ "text": "went", "original": "go", "type": "change", "reason": "Past tense", "category": "grammar" }},
      {{ "text": " to the ", "original": "", "type": "change", "reason": "Preposition", "category": "grammar" }},
      {{ "text": "store.", "type": "unchanged" }},
      {{ "text": "\\n\\n", "type": "unchanged" }},
      {{ "text": "It was fun.", "type": "unchanged" }}
    ]

    Return strictly JSON.
    """
    
    # Define a partial schema for response to match WritingResult structure but without 'mode' which we set manually
    class WritingResponseSchema(BaseModel):
        generalFeedback: str
        segments: List[WritingResult.model_fields['segments'].annotation]

    try:
        response = await client.aio.models.generate_content(
            model=model,
            contents=prompt,
            config=types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=WritingResult, # Using the full WritingResult schema, hoping Gemini fills 'mode' or we override it
                thinking_config=types.ThinkingConfig(thinking_budget=thinking_budget) if thinking_budget > 0 else None,
            )
        )

        if not response.parsed:
            raise ValueError("Empty response")
            
        result = response.parsed
        # Ensure mode matches request
        result.mode = mode
        return result

    except Exception as e:
        print(f"Writing Evaluation API Error: {e}")
        raise Exception("å†™ä½œåˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç¨åå†è¯•ã€‚")


async def chat_service(request: ChatRequest) -> str:
    context_instruction = ""
    if request.contextType == 'sentence':
         context_instruction = f'**å½“å‰æ­£åœ¨åˆ†æçš„å¥å­**: "{request.contextContent or "ç”¨æˆ·æš‚æœªè¾“å…¥å¥å­"}"ã€‚'
    elif request.contextType == 'word':
         context_instruction = f'**å½“å‰æ­£åœ¨æŸ¥è¯¢çš„å•è¯/è¯ç»„**: "{request.contextContent or "ç”¨æˆ·æš‚æœªæŸ¥è¯¢å•è¯"}"ã€‚'
    elif request.contextType == 'writing':
         context_instruction = f'**å½“å‰æ­£åœ¨æ¶¦è‰²çš„æ–‡ç« **: "{request.contextContent or "ç”¨æˆ·æš‚æœªè¾“å…¥æ–‡ç« "}"ã€‚'

    system_instruction = f"""
        ä½ æ˜¯ä¸€ä¸ªçƒ­æƒ…ã€ä¸“ä¸šçš„è‹±è¯­å­¦ä¹ åŠ©æ•™ã€‚
        
        {context_instruction}
        
        **ä½ çš„ä»»åŠ¡**ï¼š
        1. è§£ç­”ç”¨æˆ·å…³äºè‹±è¯­è¯­æ³•ã€å•è¯ç”¨æ³•ã€å¥å­ç»“æ„æˆ–è¯æ±‡è¾¨æçš„é—®é¢˜ã€‚
        2. **å§‹ç»ˆä½¿ç”¨ä¸­æ–‡**å›ç­”ã€‚
        3. ä½¿ç”¨ **Markdown** æ ¼å¼æ¥ç¾åŒ–ä½ çš„å›ç­”ï¼Œä½¿å…¶æ¸…æ™°æ˜“è¯»ï¼š
           - ä½¿ç”¨ **åŠ ç²—** æ¥å¼ºè°ƒé‡ç‚¹å•è¯æˆ–è¯­æ³•æœ¯è¯­ã€‚
           - ä½¿ç”¨åˆ—è¡¨ï¼ˆ1. æˆ– -ï¼‰æ¥åˆ†ç‚¹è§£é‡Šã€‚
           - é€‚å½“åˆ†æ®µã€‚
        4. è¯­æ°”è¦é¼“åŠ±ã€ç§¯æï¼Œåƒä¸€ä½è€å¿ƒçš„è€å¸ˆã€‚
        5. **ç‰¹æ®ŠæŒ‡ä»¤**ï¼šå¦‚æœç”¨æˆ·è¯¢é—®ç±»ä¼¼ "pop us back" è¿™æ ·çš„çŸ­è¯­ï¼Œè¯·è§£é‡Šè¿™æ˜¯ä¸€ç§å£è¯­è¡¨è¾¾ï¼Œæ ¸å¿ƒæ˜¯çŸ­è¯­åŠ¨è¯ "pop back" (è¿…é€Ÿå›å»)ï¼Œ"us" æ˜¯å®¾è¯­ã€‚
    """
    
    # Reconstruct history for Gemini
    # Gemini python SDK expects a slightly different history format if using chat.sendMessage
    # But here we might just do a single turn generation with history context if we want to be stateless, 
    # OR use the chat session. Given FastAPI is stateless, we should probably pass the history.
    # However, the `google.genai` SDK `chats.create` creates a session. 
    # We can manually construct the `contents` list from history + new message.
    
    contents = []
    for msg in request.history:
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå°† 'assistant' è½¬æ¢ä¸º Gemini æœŸæœ›çš„ 'model'
        role = 'model' if msg.role == 'assistant' else msg.role
        contents.append(types.Content(role=role, parts=[types.Part(text=msg.content)]))
    
    # Add user's new message
    contents.append(types.Content(role='user', parts=[types.Part(text=request.userMessage)]))

    try:
        response = await client.aio.models.generate_content(
            model='gemini-2.5-flash-lite',
            contents=contents,
            config=types.GenerateContentConfig(
                system_instruction=system_instruction
            )
        )
        return response.text
    except Exception as e:
        print(f"Chat API Error: {e}")
        raise Exception("èŠå¤©æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚")

